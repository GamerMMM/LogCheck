import re
import concurrent.futures
from typing import List, Dict, Tuple, Optional, NamedTuple, Set
from dataclasses import dataclass
from threading import Lock
import time
import multiprocessing as mp
from functools import partial

@dataclass
class SearchOptions:
    """æœç´¢é€‰é¡¹é…ç½®"""
    show_only: bool = False
    ignore_alpha: bool = False
    whole_pair: bool = False

class SearchResult(NamedTuple):
    """æœç´¢ç»“æœæ•°æ®ç»“æ„"""
    matched_lines: List[Tuple[int, str]]  # (è¡Œå·, è¡Œå†…å®¹)
    total_matches: int
    filtered_lines: List[Tuple[int, str]]  # è¿‡æ»¤åçš„è¡Œ
    include_pattern: str
    exclude_pattern: str
    statistics: Dict[str, int]
    search_time: float = 0.0
    highlight_ranges: List[Tuple[int, int, int]] = None  # (è¡Œå·, å¼€å§‹ä½ç½®, ç»“æŸä½ç½®)

class LineSearchResult(NamedTuple):
    """å•è¡Œæœç´¢ç»“æœ"""
    line_idx: int
    content: str
    is_match: bool
    highlight_positions: List[Tuple[int, int]]  # [(start, end), ...]

class ChunkSearchResult(NamedTuple):
    """åˆ†å—æœç´¢ç»“æœ"""
    matched_lines: List[Tuple[int, str]]
    filtered_lines: List[Tuple[int, str]]
    total_matches: int
    highlight_ranges: List[Tuple[int, int, int]]
    statistics: Dict[str, int]

class FilterEngine:
    """çœŸæ­£çš„é«˜æ€§èƒ½å¹¶è¡Œæœç´¢å¼•æ“"""
    
    def __init__(self, max_workers: Optional[int] = None):
        # ä½¿ç”¨CPUæ ¸å¿ƒæ•°ï¼Œä½†é™åˆ¶æœ€å¤§å€¼é¿å…è¿‡åº¦åˆ›å»ºçº¿ç¨‹
        self.max_workers = min(max_workers or mp.cpu_count(), 8)
        self._search_cache = {}
        self._cache_lock = Lock()
        self._last_search_time = 0.0
        
        # æ€§èƒ½å‚æ•°
        self.MIN_CHUNK_SIZE = 50  # æœ€å°åˆ†å—å¤§å°
        self.MAX_CHUNK_SIZE = 500  # æœ€å¤§åˆ†å—å¤§å°
        self.SMALL_FILE_THRESHOLD = 1000  # å°æ–‡ä»¶é˜ˆå€¼ï¼Œä¸è¿›è¡Œå¹¶è¡Œå¤„ç†

    def apply(self, editor, include_keywords: List[str], exclude_keywords: List[str],
              show_only: bool, ignore_alpha: bool, whole_pair: bool):
        """ä¸»è¦æœç´¢æ¥å£ - çœŸæ­£çš„å¹¶è¡ŒåŒ–å¤„ç†"""
        print("ğŸš€ å¯åŠ¨é«˜æ€§èƒ½å¹¶è¡Œæœç´¢...")
        start_time = time.time()
        
        options = SearchOptions(show_only, ignore_alpha, whole_pair)
        text_content = editor.toPlainText()
        
        # æ‰§è¡ŒçœŸæ­£çš„å¹¶è¡Œæœç´¢
        search_result = self.parallel_search_text(text_content, include_keywords, exclude_keywords, options)
        
        # åœ¨ä¸»çº¿ç¨‹ä¸­åº”ç”¨é«˜äº®ç»“æœ
        self._apply_parallel_highlights(editor, search_result, options)
        
        total_time = time.time() - start_time
        self._last_search_time = total_time
        
        self._print_performance_stats(search_result, total_time)
        
    def parallel_search_text(self, text_content: str, include_keywords: List[str], 
                           exclude_keywords: List[str], options: SearchOptions) -> SearchResult:
        """çœŸæ­£çš„å¹¶è¡Œæœç´¢å®ç°"""
        search_start_time = time.time()
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = self._generate_cache_key(text_content, include_keywords, exclude_keywords, options)
        with self._cache_lock:
            if cache_key in self._search_cache:
                cached_result = self._search_cache[cache_key]
                print("âœ… ç¼“å­˜å‘½ä¸­")
                return SearchResult(*cached_result[:-2], search_time=0.0, highlight_ranges=cached_result[-1])
        
        lines = text_content.splitlines()
        total_lines = len(lines)
        
        if total_lines == 0:
            return SearchResult([], 0, [], "", "", {}, 0.0, [])
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        include_pattern = self.get_regex(include_keywords, options.ignore_alpha, options.whole_pair)
        exclude_pattern = self.get_regex(exclude_keywords, options.ignore_alpha, options.whole_pair)
        
        include_regex = re.compile(include_pattern, re.IGNORECASE if options.ignore_alpha else 0) if include_pattern else None
        exclude_regex = re.compile(exclude_pattern, re.IGNORECASE if options.ignore_alpha else 0) if exclude_pattern else None
        
        # æ ¹æ®æ–‡ä»¶å¤§å°é€‰æ‹©å¤„ç†ç­–ç•¥
        if total_lines < self.SMALL_FILE_THRESHOLD:
            print(f"ğŸ“„ å°æ–‡ä»¶å¤„ç† ({total_lines} è¡Œ)")
            result = self._search_lines_sequential(lines, include_regex, exclude_regex, options)
        else:
            print(f"ğŸ”¥ å¤§æ–‡ä»¶å¹¶è¡Œå¤„ç† ({total_lines} è¡Œ)")
            result = self._search_lines_parallel(lines, include_regex, exclude_regex, options)
        
        search_time = time.time() - search_start_time
        
        # æ„å»ºæœ€ç»ˆç»“æœ
        final_result = SearchResult(
            matched_lines=result.matched_lines,
            total_matches=result.total_matches,
            filtered_lines=result.filtered_lines,
            include_pattern=include_pattern,
            exclude_pattern=exclude_pattern,
            statistics=result.statistics,
            search_time=search_time,
            highlight_ranges=result.highlight_ranges
        )
        
        # ç¼“å­˜ç»“æœ
        with self._cache_lock:
            self._search_cache[cache_key] = (
                result.matched_lines, result.total_matches, result.filtered_lines,
                include_pattern, exclude_pattern, result.statistics, result.highlight_ranges
            )
        
        return final_result

    def _search_lines_parallel(self, lines: List[str], include_regex: Optional[re.Pattern], 
                             exclude_regex: Optional[re.Pattern], options: SearchOptions) -> ChunkSearchResult:
        """çœŸæ­£çš„å¹¶è¡Œæœç´¢å®ç° - å¤šè¿›ç¨‹/çº¿ç¨‹å¤„ç†"""
        total_lines = len(lines)
        
        # æ™ºèƒ½åˆ†å—ï¼šæ ¹æ®CPUæ ¸å¿ƒæ•°å’Œæ–‡ä»¶å¤§å°åŠ¨æ€è°ƒæ•´
        optimal_chunk_size = max(
            self.MIN_CHUNK_SIZE,
            min(self.MAX_CHUNK_SIZE, total_lines // (self.max_workers * 2))
        )
        
        chunks = []
        for i in range(0, total_lines, optimal_chunk_size):
            end_idx = min(i + optimal_chunk_size, total_lines)
            chunks.append((lines[i:end_idx], i))
        
        print(f"ğŸ“Š åˆ†å—ç­–ç•¥: {len(chunks)} ä¸ªå—ï¼Œæ¯å—çº¦ {optimal_chunk_size} è¡Œ")
        
        all_matched_lines = []
        all_filtered_lines = []
        all_highlight_ranges = []
        total_matches = 0
        statistics = {"total_lines": total_lines, "processed_chunks": len(chunks), "chunk_size": optimal_chunk_size}
        
        # ä½¿ç”¨ThreadPoolExecutorè¿›è¡ŒçœŸæ­£çš„å¹¶è¡Œå¤„ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # åˆ›å»ºæœç´¢ä»»åŠ¡ - ä½¿ç”¨åå‡½æ•°é¿å…é‡å¤ä¼ å‚
            search_func = partial(
                self._search_chunk_worker,
                include_regex=include_regex,
                exclude_regex=exclude_regex,
                options=options
            )
            
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_chunk = {
                executor.submit(search_func, chunk_lines, start_idx): (chunk_lines, start_idx)
                for chunk_lines, start_idx in chunks
            }
            
            # æ”¶é›†ç»“æœ - ä½¿ç”¨as_completedç¡®ä¿æœ€å¿«å®Œæˆçš„ä»»åŠ¡å…ˆå¤„ç†
            completed_chunks = 0
            for future in concurrent.futures.as_completed(future_to_chunk):
                try:
                    chunk_result = future.result()
                    
                    # åˆå¹¶ç»“æœ
                    all_matched_lines.extend(chunk_result.matched_lines)
                    all_filtered_lines.extend(chunk_result.filtered_lines)
                    all_highlight_ranges.extend(chunk_result.highlight_ranges)
                    total_matches += chunk_result.total_matches
                    
                    # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
                    for key, value in chunk_result.statistics.items():
                        if key in statistics:
                            statistics[key] += value
                        else:
                            statistics[key] = value
                    
                    completed_chunks += 1
                    if completed_chunks % max(1, len(chunks) // 10) == 0:
                        print(f"â³ å¤„ç†è¿›åº¦: {completed_chunks}/{len(chunks)} å—å®Œæˆ")
                        
                except Exception as e:
                    print(f"âŒ æœç´¢å—å¤„ç†é”™è¯¯: {e}")
        
        # æŒ‰è¡Œå·æ’åºç»“æœ
        all_matched_lines.sort(key=lambda x: x[0])
        all_filtered_lines.sort(key=lambda x: x[0])
        all_highlight_ranges.sort(key=lambda x: (x[0], x[1]))
        
        return ChunkSearchResult(
            matched_lines=all_matched_lines,
            filtered_lines=all_filtered_lines,
            total_matches=total_matches,
            highlight_ranges=all_highlight_ranges,
            statistics=statistics
        )

    def _search_chunk_worker(self, chunk_lines: List[str], start_line_idx: int,
                           include_regex: Optional[re.Pattern], exclude_regex: Optional[re.Pattern],
                           options: SearchOptions) -> ChunkSearchResult:
        """å¹¶è¡Œå·¥ä½œçº¿ç¨‹ - å¤„ç†å•ä¸ªæ–‡æœ¬å—"""
        matched_lines = []
        filtered_lines = []
        highlight_ranges = []
        total_matches = 0
        
        for i, line in enumerate(chunk_lines):
            actual_line_idx = start_line_idx + i
            line_stripped = line.strip()
            
            if not line_stripped:
                continue
            
            # æ£€æŸ¥åŒ…å«æ¡ä»¶å¹¶è®°å½•é«˜äº®ä½ç½®
            include_match = True
            include_positions = []
            if include_regex:
                matches = list(include_regex.finditer(line))
                include_match = len(matches) > 0
                include_positions = [(m.start(), m.end()) for m in matches]
            
            # æ£€æŸ¥æ’é™¤æ¡ä»¶
            exclude_match = False
            if exclude_regex:
                exclude_match = bool(exclude_regex.search(line))
            
            # åº”ç”¨è¿‡æ»¤é€»è¾‘
            if include_match and not exclude_match:
                matched_lines.append((actual_line_idx, line))
                total_matches += 1
                
                # è®°å½•é«˜äº®èŒƒå›´
                for start_pos, end_pos in include_positions:
                    highlight_ranges.append((actual_line_idx, start_pos, end_pos))
                
                if options.show_only:
                    filtered_lines.append((actual_line_idx, line))
            elif not options.show_only:
                filtered_lines.append((actual_line_idx, line))
        
        statistics = {
            "processed_lines": len(chunk_lines),
            "matched_lines": len(matched_lines),
            "total_matches": total_matches
        }
        
        return ChunkSearchResult(
            matched_lines=matched_lines,
            filtered_lines=filtered_lines,
            total_matches=total_matches,
            highlight_ranges=highlight_ranges,
            statistics=statistics
        )

    def _search_lines_sequential(self, lines: List[str], include_regex: Optional[re.Pattern], 
                               exclude_regex: Optional[re.Pattern], options: SearchOptions) -> ChunkSearchResult:
        """å°æ–‡ä»¶çš„é¡ºåºæœç´¢"""
        return self._search_chunk_worker(lines, 0, include_regex, exclude_regex, options)

    def _apply_parallel_highlights(self, editor, result: SearchResult, options: SearchOptions):
        """å¹¶è¡Œåº”ç”¨é«˜äº®ç»“æœåˆ°ç¼–è¾‘å™¨"""
        try:
            # å¦‚æœç¼–è¾‘å™¨æ”¯æŒæ‰¹é‡é«˜äº®ï¼Œä½¿ç”¨æ‰¹é‡æ–¹æ³•
            if hasattr(editor, 'apply_batch_highlights'):
                editor.apply_batch_highlights(
                    highlight_ranges=result.highlight_ranges,
                    matched_lines=result.matched_lines,
                    show_only_matches=options.show_only
                )
            else:
                # å›é€€åˆ°åŸæœ‰æ–¹æ³•
                include_keywords = self._extract_keywords_from_pattern(result.include_pattern)
                exclude_keywords = self._extract_keywords_from_pattern(result.exclude_pattern)
                
                editor.search_and_highlight(
                    include_keywords=include_keywords,
                    exclude_keywords=exclude_keywords,
                    show_only_matches=options.show_only,
                    ignore_alpha=options.ignore_alpha,
                    whole_pair=options.whole_pair
                )
            
            # è®¾ç½®æœç´¢ç»“æœ
            if hasattr(editor, 'set_search_results'):
                editor.set_search_results(result.matched_lines, result.total_matches)
                
        except Exception as e:
            print(f"âŒ åº”ç”¨æœç´¢ç»“æœåˆ°ç¼–è¾‘å™¨æ—¶å‡ºé”™: {e}")

    def batch_search_multiple_texts(self, text_contents: List[str], include_keywords: List[str], 
                                  exclude_keywords: List[str], options: SearchOptions) -> List[SearchResult]:
        """çœŸæ­£çš„å¤šæ–‡æœ¬å¹¶è¡Œæœç´¢"""
        if not text_contents:
            return []
        
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡æœç´¢ {len(text_contents)} ä¸ªæ–‡æœ¬...")
        
        # ä½¿ç”¨è¿›ç¨‹æ± è¿›è¡Œæ›´é«˜æ•ˆçš„å¹¶è¡Œå¤„ç†
        with concurrent.futures.ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # åˆ›å»ºæœç´¢ä»»åŠ¡
            search_func = partial(
                self._search_single_text_worker,
                include_keywords=include_keywords,
                exclude_keywords=exclude_keywords,
                options=options
            )
            
            futures = [executor.submit(search_func, text) for text in text_contents]
            
            results = []
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                try:
                    result = future.result()
                    results.append(result)
                    print(f"âœ… æ–‡æœ¬ {i+1}/{len(text_contents)} æœç´¢å®Œæˆ")
                except Exception as e:
                    print(f"âŒ æ‰¹é‡æœç´¢é”™è¯¯: {e}")
                    results.append(SearchResult([], 0, [], "", "", {}, 0.0, []))
            
            return results

    def _search_single_text_worker(self, text_content: str, include_keywords: List[str], 
                                 exclude_keywords: List[str], options: SearchOptions) -> SearchResult:
        """å•æ–‡æœ¬æœç´¢å·¥ä½œå‡½æ•° - ç”¨äºè¿›ç¨‹æ± """
        # åˆ›å»ºä¸´æ—¶å¼•æ“å®ä¾‹ï¼ˆé¿å…è¿›ç¨‹é—´å…±äº«é—®é¢˜ï¼‰
        temp_engine = HighPerformanceFilterEngine(max_workers=2)  # è¿›ç¨‹å†…ä½¿ç”¨è¾ƒå°‘çº¿ç¨‹
        return temp_engine.parallel_search_text(text_content, include_keywords, exclude_keywords, options)

    def get_regex(self, keywords: List[str], ignore_case: bool = False, whole_word: bool = False) -> str:
        """ä¼˜åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼ç”Ÿæˆ"""
        if not keywords:
            return ""
        
        # è¿‡æ»¤å¹¶é¢„å¤„ç†å…³é”®è¯
        processed_keywords = []
        for kw in keywords:
            kw = kw.strip()
            if not kw:
                continue
            
            # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
            escaped = re.escape(kw)
            
            if whole_word:
                # æ•´è¯åŒ¹é… - ä¼˜åŒ–è¾¹ç•Œæ£€æµ‹
                escaped = r'\b' + escaped + r'\b'
            
            processed_keywords.append(escaped)
        
        if not processed_keywords:
            return ""
        
        # ä½¿ç”¨éæ•è·ç»„ä¼˜åŒ–æ€§èƒ½
        pattern = "(?:" + "|".join(processed_keywords) + ")"
        return pattern

    def measure_performance_comprehensive(self, text_content: str, include_keywords: List[str], 
                                        exclude_keywords: List[str], options: SearchOptions, 
                                        iterations: int = 5) -> Dict[str, float]:
        """å…¨é¢çš„æ€§èƒ½æµ‹è¯•"""
        print(f"ğŸ”¬ å¼€å§‹å…¨é¢æ€§èƒ½æµ‹è¯• - {iterations} æ¬¡è¿­ä»£...")
        
        times = []
        cache_hit_times = []
        
        for i in range(iterations):
            # æµ‹è¯•æ— ç¼“å­˜æ€§èƒ½
            self.clear_cache()
            start_time = time.time()
            result = self.parallel_search_text(text_content, include_keywords, exclude_keywords, options)
            no_cache_time = time.time() - start_time
            times.append(no_cache_time)
            
            # æµ‹è¯•ç¼“å­˜å‘½ä¸­æ€§èƒ½
            start_time = time.time()
            cached_result = self.parallel_search_text(text_content, include_keywords, exclude_keywords, options)
            cache_time = time.time() - start_time
            cache_hit_times.append(cache_time)
            
            print(f"   ç¬¬ {i+1} æ¬¡: æ— ç¼“å­˜ {no_cache_time:.3f}s, ç¼“å­˜ {cache_time:.3f}s, åŒ¹é… {len(result.matched_lines)} è¡Œ")
        
        stats = {
            "avg_no_cache_time": sum(times) / len(times),
            "min_no_cache_time": min(times),
            "max_no_cache_time": max(times),
            "avg_cache_hit_time": sum(cache_hit_times) / len(cache_hit_times),
            "cache_speedup": (sum(times) / len(times)) / (sum(cache_hit_times) / len(cache_hit_times)),
            "total_lines": len(text_content.splitlines()),
            "matched_lines": len(result.matched_lines),
            "workers_used": self.max_workers
        }
        
        print(f"ğŸ“ˆ æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"   - å¹³å‡æœç´¢æ—¶é—´ (æ— ç¼“å­˜): {stats['avg_no_cache_time']:.3f}s")
        print(f"   - å¹³å‡æœç´¢æ—¶é—´ (ç¼“å­˜å‘½ä¸­): {stats['avg_cache_hit_time']:.3f}s")
        print(f"   - ç¼“å­˜åŠ é€Ÿæ¯”: {stats['cache_speedup']:.1f}x")
        print(f"   - ä½¿ç”¨çº¿ç¨‹æ•°: {stats['workers_used']}")
        print(f"   - å¤„ç†æ•ˆç‡: {stats['total_lines']/stats['avg_no_cache_time']:.0f} è¡Œ/ç§’")
        
        return stats

    def _generate_cache_key(self, text_content: str, include_keywords: List[str], 
                          exclude_keywords: List[str], options: SearchOptions) -> str:
        """ä¼˜åŒ–çš„ç¼“å­˜é”®ç”Ÿæˆ"""
        # ä½¿ç”¨æ›´é«˜æ•ˆçš„å“ˆå¸Œæ–¹æ³•
        content_hash = hash(text_content)
        include_hash = hash(tuple(sorted(include_keywords)))
        exclude_hash = hash(tuple(sorted(exclude_keywords)))
        options_hash = hash((options.show_only, options.ignore_alpha, options.whole_pair))
        
        return f"{content_hash}_{include_hash}_{exclude_hash}_{options_hash}"

    def _extract_keywords_from_pattern(self, pattern: str) -> List[str]:
        """ä»æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ä¸­æå–åŸå§‹å…³é”®è¯"""
        if not pattern:
            return []
        
        # ç§»é™¤éæ•è·ç»„æ ‡è®°
        pattern = pattern.replace('(?:', '').replace(')', '')
        
        keywords = []
        for part in pattern.split('|'):
            # ç§»é™¤è¯è¾¹ç•Œå’Œè½¬ä¹‰å­—ç¬¦
            cleaned = part.replace(r'\b', '')
            cleaned = re.sub(r'\\(.)', r'\1', cleaned)
            if cleaned.strip():
                keywords.append(cleaned.strip())
        
        return keywords

    def _print_performance_stats(self, result: SearchResult, total_time: float):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        print(f"ğŸ¯ æœç´¢æ€§èƒ½ç»Ÿè®¡:")
        print(f"   - æ€»è€—æ—¶: {total_time:.3f}ç§’")
        print(f"   - æœç´¢å¤„ç†æ—¶é—´: {result.search_time:.3f}ç§’")
        print(f"   - UIåº”ç”¨æ—¶é—´: {total_time - result.search_time:.3f}ç§’")
        print(f"   - åŒ¹é…è¡Œæ•°: {len(result.matched_lines)}")
        print(f"   - æ€»è¡Œæ•°: {result.statistics.get('total_lines', 0)}")
        print(f"   - å¤„ç†æ•ˆç‡: {result.statistics.get('total_lines', 0)/result.search_time:.0f} è¡Œ/ç§’")
        print(f"   - ä½¿ç”¨çº¿ç¨‹æ•°: {self.max_workers}")
        
        if 'processed_chunks' in result.statistics:
            print(f"   - å¤„ç†å—æ•°: {result.statistics['processed_chunks']}")
            print(f"   - å¹³å‡å—å¤§å°: {result.statistics.get('chunk_size', 0)} è¡Œ")

    def clear_cache(self):
        """çº¿ç¨‹å®‰å…¨çš„ç¼“å­˜æ¸…ç†"""
        with self._cache_lock:
            self._search_cache.clear()

    def get_cache_stats(self) -> Dict[str, int]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self._cache_lock:
            return {
                "cache_size": len(self._search_cache),
                "max_workers": self.max_workers,
                "cache_memory_mb": sum(len(str(v)) for v in self._search_cache.values()) / (1024*1024)
            }

    def get_last_search_time(self) -> float:
        """è·å–æœ€åä¸€æ¬¡æœç´¢çš„è€—æ—¶"""
        return self._last_search_time